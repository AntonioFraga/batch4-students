{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bdd5aa2da3067e63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# SLU08 - Classification With Logistic Regression: Exercise notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e2a09fb80383b343",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27488522ce0b142e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this notebook you will practice the following: \n",
    "\n",
    "    - What classification is for\n",
    "    - Logistic regression\n",
    "    - Cost function\n",
    "    - Binary classification\n",
    "    \n",
    "You thought that you would get away without implementing your own little Logistic Regression? Hah!\n",
    "\n",
    "\n",
    "# Exercise 1. Implement the Exponential part of Sigmoid Function\n",
    "\n",
    "\n",
    "In the first exercise you will implement a bit of the sigmoid function. \n",
    "\n",
    "Here's a quick reminder of the formula:\n",
    "\n",
    "$$\\hat{p} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "In this exercise we only want you to complete this bit: $$e^{-z}$$\n",
    "\n",
    "Where z will be a two variable linear equation + the intercept: \n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n",
    "\n",
    "**Hint: Divide youe z into pieces by Betas, I've left the placeholders in there!**\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad53d6e01c034eec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9f387e5a9aa527fc",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "value_arr = [1, 1, 2, 2, 0.4]\n",
    "\n",
    "exponential= exponential_z(\n",
    "    value_arr[0], value_arr[1], value_arr[2], value_arr[3], value_arr[4])\n",
    "\n",
    "\n",
    "expected_hash_1 = 'dde84a2ea5e05e536408c4ae10402420ebd7d8ee1bce6028ac9e0752914891f5'\n",
    "assert hashlib.sha256(str(exponential).encode('utf-8')).hexdigest() == expected_hash_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b318c632b90c5f18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Exponential part: 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f40cf4a6bb066df",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Make a Prediction\n",
    "\n",
    "The next step is to implement a function that receives an observation and returns the predicted probability.\n",
    "\n",
    "For instance:\n",
    "\n",
    "$$\\hat{p} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where Z is the linear equation - you can't use the same function that you used above for the Z part as the input are now two arrays, one with the train example and another with the coefficients.\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4207cb1317b5cea0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9f387e5a9aa527fc",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([-1, -1])\n",
    "coefficients = np.array([0 ,3.2, -1])\n",
    "\n",
    "expected_hash_2 = 'aef234a36241b1803c64b1e12a9c77eb4934592967c002eafb0802c2f4e3abf7'\n",
    "assert hashlib.sha256(str(predict_proba(x, coefficients)).encode('utf-8')).hexdigest() == expected_hash_2\n",
    "\n",
    "x_1 = np.array([-1, -1, 2, 0])\n",
    "coefficients_1 = np.array([0 ,2, -1, 0.2, 0])\n",
    "\n",
    "expected_hash_3 = '15430ee47ba42ef9fd64b5a3aab25407a23c1545c6479b91751f7761e809db7f'\n",
    "assert hashlib.sha256(str(predict_proba(x_1, coefficients_1)).encode('utf-8')).hexdigest() == expected_hash_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c2afb353cd17406e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Predicted probabilities for example with 2 variables:  0.0975\n",
    "    \n",
    "    Predicted probabilities for example with 3 variables:  0.3545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6444271eaf86b4f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 3: Compute the Maximum Log-Likelihood Cost Function\n",
    "\n",
    "As you will implement stochastic gradient descent, you only have to do the following for each prediction: \n",
    "\n",
    "$$H_{\\hat{p}}(y) =  - (y \\log(\\hat{p}) + (1-y) \\log (1-\\hat{p}))$$\n",
    "\n",
    "In the next exercise you will loop through some examples stored in a array and calculate the cost function for the full dataset. Recall that the formula to generalize the cost function across several examples is: \n",
    "\n",
    "$$H_{\\hat{p}}(y) = - \\frac{1}{N}\\sum_{i=1}^{N} \\left [{ y_i \\ \\log(\\hat{p}_i) + (1-y_i) \\ \\log (1-\\hat{p}_i)} \\right ]$$\n",
    "\n",
    "You will basically simulate what stochastic gradient descent does - computing the log for each example, sum each log-loss and then averaging the result across the number of observations in the x dataset/array.\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-daa84c9da629c861",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9f387e5a9aa527fc",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[-1, -1], [3, 0], [3, 2]])\n",
    "coefficients = np.array([[0 ,2, -1]])\n",
    "y = np.array([[1],[1],[0]])\n",
    "expected_hash_4 = '5f34754c780fd8925a4f86ebcb782c31eb6af9b97ce71bce746c43df2fea1c4d'\n",
    "assert hashlib.sha256(str(cost_function(x, coefficients, y)).encode('utf-8')).hexdigest() == expected_hash_4\n",
    "\n",
    "x_1 = np.array([[-1, -1], [3, 0], [3, 2], [1, 0]])\n",
    "y_1 = np.array([[1],[1],[0],[1]])\n",
    "\n",
    "expected_hash_5 = '8aedf0c2ad0b89e903585870d0c372fd485d84e2464f1ea01191f9a0ddc49a9c'\n",
    "assert hashlib.sha256(str(cost_function(x_1, coefficients, y_1)).encode('utf-8')).hexdigest() == expected_hash_5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73aa2b5fc2e95825",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Computed log loss for first training set:  1.77796243\n",
    "    \n",
    "    Computed log loss for second training set:  1.36520383"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb7dca3ebe6d82c8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: Compute a first pass on Stochastic Gradient Descent\n",
    "\n",
    "Now that the warmup is done, let's do the most interesting exercise - computing the derivatives and updating our coefficients. Here you will do a full pass a bunch of examples, computing the gradient descent for each one of them.\n",
    "\n",
    "In this exercise, you should compute a single iteration of the gradient descent! \n",
    "\n",
    "## Quick reminders:\n",
    "\n",
    "Remember our formulas for the gradient:\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_{0(t)}}$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_t}$$\n",
    "\n",
    "which can be simplified to\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p})\\right]$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p}) \\ x \\right]$$\n",
    "\n",
    "You will have to initialize the coefficients in some way. If you have a training set $X$, you can initialize it them to zero, this way:\n",
    "```python\n",
    "coefficients = np.zeros(X.shape[1]+1)\n",
    "```\n",
    "\n",
    "where the $+1$ is adding the intercept.\n",
    "\n",
    "Note: We are doing a stochastic gradient descent so don't forget to go observation by observation and updating the coefficients everytime!\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f387e5a9aa527fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_coefficients(x_train, y_train, learning_rate = 0.1, verbose = False):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns the a first iteration of \n",
    "    batch gradient descent\n",
    "\n",
    "    Args:\n",
    "        x_train (np.array): a numpy array of shape (m, n)\n",
    "            m: number of training observations\n",
    "            n: number of variables\n",
    "        y_train (np.array): a numpy array of shape (m,) with \n",
    "        the real value of the target\n",
    "        learning_rate (np.float64): a float\n",
    "\n",
    "    Returns:\n",
    "        coefficients (np.array): a numpy array of shape (n+1,)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of observations\n",
    "    m = x_train.shape[0]\n",
    "    \n",
    "    # Number of variables \n",
    "    n = x_train.shape[1]\n",
    "\n",
    "    # initialize the coefficients array with zeros\n",
    "    # hint: use np.zeros()\n",
    "    coefficients = np.zeros(x_train.shape[1]+1)\n",
    "    \n",
    "    # run the stochastic gradient descent and update the coefficients after \n",
    "    # each observation\n",
    "    for i in range(m):                  \n",
    "        # compute the predicted probability - you can use a function we have done previously - don't forget about\n",
    "        # intercept!\n",
    "        \n",
    "        observation_i = x_train[i]\n",
    "        proba = predict_proba(observation_i, coefficients)                    \n",
    "        # Update intercept:\n",
    "        coefficients[0] += learning_rate * (y_train[i]-proba)*proba*(1-proba)        \n",
    "        # Update the rest of the coefficients by looping through the variables\n",
    "        for col in range(n):\n",
    "            coefficients[col+1] += learning_rate * (y_train[i]-proba)*proba*(1-proba)*x_train[i, col]    \n",
    "    \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b95087a05a56d10",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array([[1,2,3], [2,5,9], [3,1,4], [8,2,9]])\n",
    "y_train = np.array([0,1,0,1])\n",
    "learning_rate = 0.1\n",
    "\n",
    "expected_hash_6 = 'f95e17d924a3e26cae40f026a07c5060de2de80029c1942ee1f3c8a3a9f13d20'\n",
    "assert hashlib.sha256(str(compute_coefficients(x_train, y_train, learning_rate)[0]).encode('utf-8')).hexdigest() == expected_hash_6\n",
    "\n",
    "expected_hash_7 = '7e40fa32e8fbfd16907a8d5e2c388a88515cd95a38e4e1d1f15f8620ffdcb077'\n",
    "assert hashlib.sha256(str(compute_coefficients(x_train, y_train, learning_rate)[1]).encode('utf-8')).hexdigest() == expected_hash_7\n",
    "\n",
    "expected_hash_8 = 'f45991f62e49913d9887d333bdf7d2c973ba74d32ed440b64224acb7f5a2de36'\n",
    "assert hashlib.sha256(str(compute_coefficients(x_train, y_train, learning_rate)[2]).encode('utf-8')).hexdigest() == expected_hash_8\n",
    "\n",
    "expected_hash_9 = '1882ce141295f7bdf9685a4243521842d835dba9485b503da4a1a22a3a4b2384'\n",
    "assert hashlib.sha256(str(compute_coefficients(x_train, y_train, learning_rate)[3]).encode('utf-8')).hexdigest() == expected_hash_9\n",
    "\n",
    "x_train_1 = np.array([[4,5,2,7], [2,5,7,2], [3,1,2,1], [8,2,9,5], [1,2,9,4]])\n",
    "y_train_1 = np.array([0,1,0,1,1])\n",
    "\n",
    "expected_array = '9bf566f2c5521b165b3cb8bc67b7b2bea3c36f791086a0ac7134e82e8a6a2b13'\n",
    "assert hashlib.sha256(compute_coefficients(x_train_1, y_train_1, learning_rate)).hexdigest() == expected_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a1387ec5d3ac2d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5: Normalize Data\n",
    "\n",
    "To get this concept in your head, let's do a quick and easy function to normalize the data using a MaxMin approach. It is crucial that your variables are adjusted between $[0;1]$ (normalized) or standardized so that you can correctly analyse some logistic regression coefficients for your possible future employer.\n",
    "\n",
    "You only have to implement this formula\n",
    "\n",
    "$$ x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Don't forget that the `axis` argument is critical when obtaining the maximum, minimum and mean values! As you want to obtain the maximum and minimum values of each individual feature, you have to specify `axis=0`. Thus, if you wanted to obtain the maximum values of each feature of data $X$, you would do the following:\n",
    "\n",
    "```python\n",
    "X_max = np.max(X, axis=0)\n",
    "```\n",
    "\n",
    "Not an assertable question but can you remember why it is important to normalize data for Logistic Regression?\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4e20d91c912030a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-334419ee9c78c698",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[7,7,3], [2,2,11], [9,5,2], [0,9,5], [10,1,3], [1,5,2]])\n",
    "normalized_data = normalize_data(data)\n",
    "print('Before normalization:')\n",
    "print(data)\n",
    "print('\\n-------------------\\n')\n",
    "print('After normalization:')\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-194d7aa04c4e007c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Before normalization:\n",
    "    [[ 7  7  3]\n",
    "     [ 2  2 11]\n",
    "     [ 9  5  2]\n",
    "     [ 0  9  5]\n",
    "     [10  1  3]\n",
    "     [ 1  5  2]]\n",
    "\n",
    "    -------------------\n",
    "\n",
    "After normalization:\n",
    "\n",
    "    [[0.7        0.75       0.11111111]\n",
    "     [0.2        0.125      1.        ]\n",
    "     [0.9        0.5        0.        ]\n",
    "     [0.         1.         0.33333333]\n",
    "     [1.         0.         0.11111111]\n",
    "     [0.1        0.5        0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b95087a05a56d10",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[2,2,11,1], [7,5,1,3], [9,5,2,6]])\n",
    "normalized_data = normalize_data(data)\n",
    "assert hashlib.md5(normalized_data).hexdigest() == '277af6f0e6721a66ca19931c726aeb86'\n",
    "\n",
    "data = np.array([[1,3,1,3], [9,5,3,1], [2,2,4,6]])\n",
    "normalized_data = normalize_data(data)\n",
    "assert hashlib.md5(normalized_data).hexdigest() == '2548d399591b9f950ab5fed9cc89a4e5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Training a Logistic Regression with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will load the Titanic dataset and try to use the available numerical variables to predict the probability of a person surviving the titanic sinking.\n",
    "\n",
    "Prepare to use your sklearn skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load the dataset for you\n",
    "titanic = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you need to do the following: \n",
    "    - Create an array with the target variable (Survived)\n",
    "    - Create an array with the X numeric variables (Pclass, Age, Siblings/Spouses Aboard, Parents/Children Aboard and Fare)\n",
    "    - Scale all the X variables.\n",
    "    - Fit a logistic regression for maximum of 100 epochs and random state = 100.\n",
    "    - Return an array of the predicted probas and return the coefficients\n",
    "    \n",
    "After this, feel free to explore your predictions! As a bonus why don't your construct a decision boundary using two variables eh? ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_model(dataset):\n",
    "    '''\n",
    "    Returns the predicted probas and coefficients \n",
    "    of a trained logistic regression on dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset(pd.DataFrame): dataset to train on.\n",
    "    \n",
    "    Returns:\n",
    "        probas (np.array): Array of floats with the probability \n",
    "        of surviving for each passenger\n",
    "        coefficients (np.array): Returned coefficients of the \n",
    "        trained logistic regression.\n",
    "    '''\n",
    "    \n",
    "    # Get the Survived variable for y\n",
    "    y = titanic.Survived\n",
    "    \n",
    "    # Select the Numerical variables for X \n",
    "    X = titanic[['Pclass','Age','Siblings/Spouses Aboard',\n",
    "       'Parents/Children Aboard', 'Fare']]\n",
    "    \n",
    "    # Scale the X dataset - you can use a function we have already\n",
    "    # constructed or resort to the sklearn implementation\n",
    "    X_norm = normalize_data(X)\n",
    "    \n",
    "    # Define logistic regression from sklearn with the hyperparameters \n",
    "    # defined above - also add random_state = 100\n",
    "    \n",
    "    # Hint: for epochs look at the max_iter hyper param!\n",
    "    lr = LogisticRegression(max_iter=100, random_state = 100)\n",
    "    \n",
    "    # Fit logistic\n",
    "    lr.fit(X_norm, y)\n",
    "    \n",
    "    # Obtain probability of surviving\n",
    "    probas = lr.predict_proba(X_norm)[:,1]\n",
    "    \n",
    "    # Obtain Coefficients from logistic regression\n",
    "    # Hint: see the sklearn logistic regression documentation\n",
    "    # if you do not know how to do this\n",
    "    # No need to return the intercept, just the variable coefficient!\n",
    "    coef = lr.coef_\n",
    "    \n",
    "    return probas, coef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas, coef = train_model(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b95087a05a56d10",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "lr_1_hash = 'fed49f7123774be4f63b89f8f05d78e2a37b813a1fcc4e594f8349589a156b75'\n",
    "assert hashlib.sha256(probas.round(2)).hexdigest() == lr_1_hash\n",
    "\n",
    "lr_2_hash = '3f90397ddc3abdec0374caf926d93d8377ec2411e05c6f6da5b17ce0d430c1c5'\n",
    "assert hashlib.sha256(coef.round(2)).hexdigest() ==  lr_2_hash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
